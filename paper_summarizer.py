#!/usr/bin/env python3
"""
arXiv Paper Summarizer
======================
arXiv URLì„ ì…ë ¥ë°›ì•„ PDFë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³ , ì„¹ì…˜ë³„ë¡œ ìš”ì•½í•˜ì—¬ Markdown í˜•íƒœë¡œ ì¶œë ¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import re
import os
import sys
import requests
from urllib.parse import urlparse
from PyPDF2 import PdfReader
from transformers import pipeline
import nltk
from typing import Dict, List, Optional

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

class ArxivPaperSummarizer:
    def __init__(self):
        """ì´ˆê¸°í™” - ìš”ì•½ ëª¨ë¸ ë¡œë“œ"""
        print("ìš”ì•½ ëª¨ë¸ì„ ë¡œë”©ì¤‘ì…ë‹ˆë‹¤...")
        self.summarizer = pipeline(
            "summarization",
            model="facebook/bart-large-cnn",
            device=-1  # CPU ì‚¬ìš©
        )
        print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    def download_pdf(self, arxiv_url: str) -> str:
        """arXiv URLì—ì„œ PDF ë‹¤ìš´ë¡œë“œ"""
        print(f"PDF ë‹¤ìš´ë¡œë“œ ì¤‘: {arxiv_url}")
        
        # URL ê²€ì¦ ë° PDF URL ë³€í™˜
        if "arxiv.org/abs/" in arxiv_url:
            arxiv_url = arxiv_url.replace("arxiv.org/abs/", "arxiv.org/pdf/")
        
        if not arxiv_url.endswith(".pdf"):
            arxiv_url += ".pdf"
        
        # PDF íŒŒì¼ëª… ìƒì„±
        paper_id = arxiv_url.split("/")[-1].replace(".pdf", "")
        pdf_filename = f"{paper_id}.pdf"
        
        # PDF ë‹¤ìš´ë¡œë“œ
        try:
            response = requests.get(arxiv_url, timeout=30)
            response.raise_for_status()
            
            with open(pdf_filename, 'wb') as f:
                f.write(response.content)
            
            print(f"PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {pdf_filename}")
            return pdf_filename
            
        except requests.exceptions.RequestException as e:
            raise Exception(f"PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        print("PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
        
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PdfReader(file)
                text = ""
                
                for page_num, page in enumerate(pdf_reader.pages):
                    try:
                        page_text = page.extract_text()
                        text += page_text + "\n"
                    except Exception as e:
                        print(f"í˜ì´ì§€ {page_num + 1} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                        continue
                
                print(f"ì´ {len(pdf_reader.pages)}í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ")
                return text
                
        except Exception as e:
            raise Exception(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    
    def parse_sections(self, text: str) -> Dict[str, str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì„¹ì…˜ë³„ë¡œ ë¶„ë¦¬"""
        print("ì„¹ì…˜ë³„ í…ìŠ¤íŠ¸ ë¶„ë¦¬ ì¤‘...")
        
        sections = {
            'abstract': '',
            'introduction': '',
            'method': '',
            'results': '',
            'conclusion': ''
        }
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = re.sub(r'\n+', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        # Abstract ì¶”ì¶œ
        abstract_patterns = [
            r'Abstract\s*(.+?)(?=1\s+Introduction|Introduction|1\.|\n\n)',
            r'ABSTRACT\s*(.+?)(?=1\s+INTRODUCTION|INTRODUCTION|1\.|\n\n)',
            r'Abstract[\s\n]*(.+?)(?=Keywords|Introduction|1\s)',
        ]
        
        for pattern in abstract_patterns:
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            if match:
                sections['abstract'] = match.group(1).strip()
                break
        
        # Introduction ì¶”ì¶œ
        intro_patterns = [
            r'(?:1\s+)?Introduction\s*(.+?)(?=2\s+|Method|Related Work|Background)',
            r'(?:1\s+)?INTRODUCTION\s*(.+?)(?=2\s+|METHOD|RELATED WORK|BACKGROUND)',
        ]
        
        for pattern in intro_patterns:
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            if match:
                sections['introduction'] = match.group(1).strip()[:2000]  # ê¸¸ì´ ì œí•œ
                break
        
        # Method ì¶”ì¶œ
        method_patterns = [
            r'(?:\d+\s+)?(?:Method|Methodology|Approach|Model)\s*(.+?)(?=\d+\s+|Result|Experiment|Evaluation|Discussion)',
            r'(?:\d+\s+)?(?:METHOD|METHODOLOGY|APPROACH|MODEL)\s*(.+?)(?=\d+\s+|RESULT|EXPERIMENT|EVALUATION|DISCUSSION)',
        ]
        
        for pattern in method_patterns:
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            if match:
                sections['method'] = match.group(1).strip()[:2000]  # ê¸¸ì´ ì œí•œ
                break
        
        # Results/Discussion ì¶”ì¶œ
        results_patterns = [
            r'(?:\d+\s+)?(?:Result|Experiment|Evaluation|Discussion)\s*(.+?)(?=\d+\s+|Conclusion|Reference|Acknowledge)',
            r'(?:\d+\s+)?(?:RESULT|EXPERIMENT|EVALUATION|DISCUSSION)\s*(.+?)(?=\d+\s+|CONCLUSION|REFERENCE|ACKNOWLEDGE)',
        ]
        
        for pattern in results_patterns:
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            if match:
                sections['results'] = match.group(1).strip()[:2000]  # ê¸¸ì´ ì œí•œ
                break
        
        # Conclusion ì¶”ì¶œ
        conclusion_patterns = [
            r'(?:\d+\s+)?Conclusion\s*(.+?)(?=Reference|Acknowledge|Appendix|\Z)',
            r'(?:\d+\s+)?CONCLUSION\s*(.+?)(?=REFERENCE|ACKNOWLEDGE|APPENDIX|\Z)',
        ]
        
        for pattern in conclusion_patterns:
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            if match:
                sections['conclusion'] = match.group(1).strip()[:1500]  # ê¸¸ì´ ì œí•œ
                break
        
        # ì¶”ì¶œëœ ì„¹ì…˜ ì •ë³´ ì¶œë ¥
        for section, content in sections.items():
            if content:
                print(f"{section.capitalize()}: {len(content)} ê¸€ì ì¶”ì¶œë¨")
            else:
                print(f"{section.capitalize()}: ì¶”ì¶œë˜ì§€ ì•ŠìŒ")
        
        return sections
    
    def summarize_text(self, text: str, max_length: int = 150, min_length: int = 50) -> str:
        """í…ìŠ¤íŠ¸ ìš”ì•½"""
        if not text or len(text) < 100:
            return text
        
        try:
            # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸° (ëª¨ë¸ ì…ë ¥ ê¸¸ì´ ì œí•œ)
            max_chunk_length = 1000
            chunks = [text[i:i+max_chunk_length] for i in range(0, len(text), max_chunk_length)]
            
            summaries = []
            for chunk in chunks:
                if len(chunk.strip()) < 50:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ëŠ” ê±´ë„ˆë›°ê¸°
                    continue
                    
                summary = self.summarizer(
                    chunk,
                    max_length=max_length,
                    min_length=min_length,
                    do_sample=False
                )
                summaries.append(summary[0]['summary_text'])
            
            return " ".join(summaries)
            
        except Exception as e:
            print(f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ìš”ì•½ ì‹¤íŒ¨ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ì˜ ì•ë¶€ë¶„ ë°˜í™˜
            sentences = nltk.sent_tokenize(text)
            return " ".join(sentences[:3]) if sentences else text[:500]
    
    def generate_markdown_summary(self, sections: Dict[str, str], arxiv_url: str) -> str:
        """ì„¹ì…˜ë³„ ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ Markdown ìš”ì•½ ìƒì„±"""
        print("Markdown ìš”ì•½ ìƒì„± ì¤‘...")
        
        # ë…¼ë¬¸ ì œëª© ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ì‹)
        title = "arXiv Paper Summary"
        
        markdown = f"""# {title}

**Source**: {arxiv_url}

---

"""
        
        # Abstract ìš”ì•½
        if sections['abstract']:
            print("Abstract ìš”ì•½ ì¤‘...")
            abstract_summary = self.summarize_text(sections['abstract'], max_length=200, min_length=80)
            markdown += f"""## ğŸ“ Abstract

{abstract_summary}

"""
        
        # Introduction ìš”ì•½
        if sections['introduction']:
            print("Introduction ìš”ì•½ ì¤‘...")
            intro_summary = self.summarize_text(sections['introduction'], max_length=180, min_length=70)
            markdown += f"""## ğŸ¯ Introduction

{intro_summary}

"""
        
        # Method ìš”ì•½
        if sections['method']:
            print("Method ìš”ì•½ ì¤‘...")
            method_summary = self.summarize_text(sections['method'], max_length=200, min_length=80)
            markdown += f"""## ğŸ”¬ Methodology

{method_summary}

"""
        
        # Results ìš”ì•½
        if sections['results']:
            print("Results ìš”ì•½ ì¤‘...")
            results_summary = self.summarize_text(sections['results'], max_length=180, min_length=70)
            markdown += f"""## ğŸ“Š Results & Discussion

{results_summary}

"""
        
        # Conclusion ìš”ì•½
        if sections['conclusion']:
            print("Conclusion ìš”ì•½ ì¤‘...")
            conclusion_summary = self.summarize_text(sections['conclusion'], max_length=150, min_length=60)
            markdown += f"""## ğŸ¯ Conclusion

{conclusion_summary}

"""
        
        markdown += f"""---

*Generated by arXiv Paper Summarizer*
"""
        
        return markdown
    
    def process_paper(self, arxiv_url: str) -> str:
        """ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        try:
            # 1. PDF ë‹¤ìš´ë¡œë“œ
            pdf_path = self.download_pdf(arxiv_url)
            
            # 2. í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = self.extract_text_from_pdf(pdf_path)
            
            # 3. ì„¹ì…˜ë³„ ë¶„ë¦¬
            sections = self.parse_sections(text)
            
            # 4. Markdown ìš”ì•½ ìƒì„±
            markdown_summary = self.generate_markdown_summary(sections, arxiv_url)
            
            # 5. ê²°ê³¼ ì €ì¥
            output_filename = f"{os.path.splitext(pdf_path)[0]}_summary.md"
            with open(output_filename, 'w', encoding='utf-8') as f:
                f.write(markdown_summary)
            
            print(f"\nâœ… ìš”ì•½ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: {output_filename}")
            
            # ì„ì‹œ PDF íŒŒì¼ ì‚­ì œ
            if os.path.exists(pdf_path):
                os.remove(pdf_path)
                print(f"ì„ì‹œ PDF íŒŒì¼ ì‚­ì œ: {pdf_path}")
            
            return markdown_summary
            
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return ""


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    if len(sys.argv) != 2:
        print("ì‚¬ìš©ë²•: python paper_summarizer.py <arxiv_url>")
        print("ì˜ˆì‹œ: python paper_summarizer.py https://arxiv.org/pdf/1706.03762")
        sys.exit(1)
    
    arxiv_url = sys.argv[1]
    
    print("ğŸš€ arXiv Paper Summarizer ì‹œì‘")
    print(f"ì²˜ë¦¬í•  ë…¼ë¬¸: {arxiv_url}")
    print("=" * 50)
    
    summarizer = ArxivPaperSummarizer()
    result = summarizer.process_paper(arxiv_url)
    
    if result:
        print("\n" + "=" * 50)
        print("ğŸ“„ ìš”ì•½ ê²°ê³¼:")
        print("=" * 50)
        print(result)
    else:
        print("ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)


if __name__ == "__main__":
    main()
