# arXiv Paper Summary

**Source**: https://arxiv.org/pdf/1706.03762

---

## üìù Abstract

The dominant sequence transduction models are based on complex recurrent or convolutional neural networks. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.

## üéØ Introduction

Recurrent neural networks have been firmly established as state of the art approaches in sequence modeling and transduction problems. Recurrent models typically factor computation along the symbol positions of the input and output sequences. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths. Recent work has achieved significant improvements in computational efficiency through factorization tricks. Attention mechanisms have become an integral part of compelling sequence modeling. In all but a few cases attention mechanisms are used in conjunction with a recurrent network. We propose the Transformer, a model architecture eschewing recurrence and relying entirely on an attention mechanism. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.

## üî¨ Methodology

The Transformer is a new simple network architecture based solely on attention mechanisms. It dispenses with recurrence and convolutions entirely. It is based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder. and decoder through an attention mechanism. The Transformer was developed at the University of California, San Diego.

## üìä Results & Discussion

Our model achieves 28. s on two machine translation tasks. These models are superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves a total of 28 s on the two tasks, which is more than twice the average for a machine translation task. The model is also significantly faster than other models on the task.

## üéØ Conclusion

The Transformer is the first sequence transduction model based entirely on attention. It replaces the recurrent layers most commonly used in encoder-decoder architectures. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. We are excited about the future of attention-based models and plan to apply them to other tasks. The code to build and evaluate our models is available on GitHub. The code is available at: http://www.tensorflow.com/ tensorflow/tensor2tensor. n and the code is also available at http:// www.tensorflow. com/ tensorsorflow/. n and  http://tensorsurvey.org/.

---

*Generated by arXiv Paper Summarizer*
